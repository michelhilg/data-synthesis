{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Unconditional Denoising Diffusion Probabilistic Model (DDPM) - Implementation\n","\n","The purpose of this notebook is to implement the Unconditional Denoising Diffusion Probabilistic Models architecture, as outlined in section 3.4.4 of the bachelor thesis.\n","\n","The code provided in this notebook was developed using the Kaggle platform.\n","\n","The code in this notebook incorporates the following sources as references:\n","\n","- https://github.com/dome272/Diffusion-Models-pytorch\n","- https://arxiv.org/pdf/2006.11239.pdf"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aVPCcTo3xiu8"},"source":["## Step 1 - Importing Dependencies\n","\n","- Importing the necessary libraries to execute the code."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:51:28.111702Z","iopub.status.busy":"2023-06-03T07:51:28.111014Z","iopub.status.idle":"2023-06-03T07:51:28.121234Z","shell.execute_reply":"2023-06-03T07:51:28.118936Z","shell.execute_reply.started":"2023-06-03T07:51:28.111660Z"},"id":"TVoX2eswxi6M","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as F\n","import pandas as pd\n","from PIL import Image\n","import torchvision\n","from torchvision.utils import make_grid\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import seaborn as sns"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 2 - Hyperparameter Settings\n","\n","- Set the HPs for the Unconditional DDPM deep generative model. Besides, also check whether a GPU is available for use."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.manual_seed(42)   # Manual seed for running\n","image_size = 64         # Image size for training the model. It will also be the size of the synthetic images created\n","batch_size = 8          # Batch size to train the model\n","workers = 2             # Number of CPU workers to process the data\n","ngpu = 1                # Number of GPU available\n","noise_steps = 300       # Number of steps in the forward adding noise process\n","epochs = 400            # Number of epochs to train the Unconditional DDPM\n","lr = 1e-5               # Learning rate of the AdamW optimizer\n","\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bOqBxNgNzBKM"},"source":["## Step 3 - Dataset Loading\n","\n","- Defining the custom class for loading the images as PyTorch dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T15:01:06.172839Z","iopub.status.busy":"2023-04-16T15:01:06.172434Z","iopub.status.idle":"2023-04-16T15:01:06.181607Z","shell.execute_reply":"2023-04-16T15:01:06.180309Z","shell.execute_reply.started":"2023-04-16T15:01:06.172800Z"},"id":"lHP60LfGzAnr","trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, labels_file, root_dir, transform=None):\n","        self.annotations = pd.read_csv(labels_file, header=None)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir, str(self.annotations.iloc[index, 1]), self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        image = image.convert(\"RGB\")\n","        label = torch.tensor(int(self.annotations.iloc[(index, 2)]))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return(image, label)\n","    \n","    def __getlabel__(self, index):\n","        label = (self.annotations.iloc[(index, 1)])        \n","\n","        return(label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- The preprocessing transformation matchs the data with the expected format from the DDPM model.\n","- The labels .cvs file should be passed in a class-wise definiton, since the model is unconditional will generate one class per time."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T15:01:06.185560Z","iopub.status.busy":"2023-04-16T15:01:06.185108Z","iopub.status.idle":"2023-04-16T15:01:06.192490Z","shell.execute_reply":"2023-04-16T15:01:06.191230Z","shell.execute_reply.started":"2023-04-16T15:01:06.185517Z"},"id":"doZ8ts__yv6Q","trusted":true},"outputs":[],"source":["preprocessing = transforms.Compose([transforms.Resize(image_size), \n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                                   ])\n","\n","labels_file = '/path/to/class/labels/csv'\n","root_dir = '/path/to/root/image/folder'\n","\n","dataset = Dataset(labels_file=labels_file, root_dir=root_dir, transform=preprocessing)\n","dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gi7FMHHfzC3z"},"source":["## Step 4 - Defining the Diffusion Tools for the Forward Process\n","\n","- Setting the noise schedule and complete forward process in the DDPM architecture.\n","- The forward process is responsible for add noise to the original image based on the noise schedule and the timestep."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:51:35.569013Z","iopub.status.busy":"2023-06-03T07:51:35.568531Z","iopub.status.idle":"2023-06-03T07:51:35.600502Z","shell.execute_reply":"2023-06-03T07:51:35.598011Z","shell.execute_reply.started":"2023-06-03T07:51:35.568969Z"},"id":"t2SdjWBVzDZT","trusted":true},"outputs":[],"source":["class Diffusion:\n","    def __init__(self, noise_steps=noise_steps, beta_start=1e-4, beta_end=0.02, img_size=256, device=device):\n","        self.noise_steps = noise_steps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.img_size = img_size\n","        self.device = device\n","\n","        self.beta = self.prepare_noise_schedule().to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n","\n","    def prepare_noise_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n","\n","    def noise_images(self, x, t):\n","        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n","        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n","        Ɛ = torch.randn_like(x)\n","        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n","\n","    def sample_timesteps(self, n):\n","        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n","\n","    def sample(self, model, n):\n","        model.eval()\n","        with torch.no_grad():\n","            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n","            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n","                t = (torch.ones(n) * i).long().to(self.device)\n","                predicted_noise = model(x, t)\n","                alpha = self.alpha[t][:, None, None, None]\n","                alpha_hat = self.alpha_hat[t][:, None, None, None]\n","                beta = self.beta[t][:, None, None, None]\n","                if i > 1:\n","                    noise = torch.randn_like(x)\n","                else:\n","                    noise = torch.zeros_like(x)\n","                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n","        model.train()\n","        x = (x.clamp(-1, 1) + 1) / 2\n","        x = (x * 255).type(torch.uint8)\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gz9H-hG70mRj"},"source":["## Step 5 - Defining the Reverse Process Neural Network (U-net Architecture)\n","\n","- This code defines the U-Net architecture responsible for the reverse process in the Unconditional DDPM model.\n","- This process has the role of removing the noise that was added to the images in the forward process."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:51:38.139004Z","iopub.status.busy":"2023-06-03T07:51:38.138418Z","iopub.status.idle":"2023-06-03T07:51:38.189832Z","shell.execute_reply":"2023-06-03T07:51:38.188342Z","shell.execute_reply.started":"2023-06-03T07:51:38.138948Z"},"id":"SoqNQ-cc0hat","trusted":true},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, channels, size):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.size = size\n","        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n","        self.ln = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n","        x_ln = self.ln(x)\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n","        super().__init__()\n","        self.residual = residual\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, mid_channels),\n","            nn.GELU(),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, out_channels),\n","        )\n","\n","    def forward(self, x):\n","        if self.residual:\n","            return F.gelu(x + self.double_conv(x))\n","        else:\n","            return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, t):\n","        x = self.maxpool_conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.conv = nn.Sequential(\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels, in_channels // 2),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, skip_x, t):\n","        x = self.up(x)\n","        x = torch.cat([skip_x, x], dim=1)\n","        x = self.conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, c_in=3, c_out=3, time_dim=256, device=device):\n","        super().__init__()\n","        self.device = device\n","        self.time_dim = time_dim\n","        self.inc = DoubleConv(c_in, 64)\n","        self.down1 = Down(64, 128)\n","        self.sa1 = SelfAttention(128, 32)\n","        self.down2 = Down(128, 256)\n","        self.sa2 = SelfAttention(256, 16)\n","        self.down3 = Down(256, 256)\n","        self.sa3 = SelfAttention(256, 8)\n","\n","        self.bot1 = DoubleConv(256, 512)\n","        self.bot2 = DoubleConv(512, 512)\n","        self.bot3 = DoubleConv(512, 256)\n","\n","        self.up1 = Up(512, 128)\n","        self.sa4 = SelfAttention(128, 16)\n","        self.up2 = Up(256, 64)\n","        self.sa5 = SelfAttention(64, 32)\n","        self.up3 = Up(128, 64)\n","        self.sa6 = SelfAttention(64, 64)\n","        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n","\n","    def pos_encoding(self, t, channels):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","        return pos_enc\n","\n","    def forward(self, x, t):\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.time_dim)\n","\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","        x4 = self.down3(x3, t)\n","        x4 = self.sa3(x4)\n","\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","\n","        x = self.up1(x4, x3, t)\n","        x = self.sa4(x)\n","        x = self.up2(x, x2, t)\n","        x = self.sa5(x)\n","        x = self.up3(x, x1, t)\n","        x = self.sa6(x)\n","        output = self.outc(x)\n","        return output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mmDR3XRA2vW8"},"source":["## Step 6 - Utils Functions for Visualization and Saving\n","\n","- `plot_images` - Function to plot the synthetic data over the trainning of the model. Util to visualize the evolution of the images over epochs.\n","- `save_synthetic_data` - Function to save the synthetic images. Work in chunks to save RAM memory since the model has to synthetize and perform the reverse process for each image that is creating."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T15:01:06.872845Z","iopub.status.busy":"2023-04-16T15:01:06.872421Z","iopub.status.idle":"2023-04-16T15:01:06.885602Z","shell.execute_reply":"2023-04-16T15:01:06.884167Z","shell.execute_reply.started":"2023-04-16T15:01:06.872808Z"},"id":"eL9yFR2W2vdV","trusted":true},"outputs":[],"source":["def plot_images(images):\n","    plt.figure(figsize=(32, 32))\n","    plt.imshow(torch.cat([\n","        torch.cat([i for i in images.cpu()], dim=-1),\n","    ], dim=-2).permute(1, 2, 0).cpu())\n","    plt.show()\n","\n","\n","def save_synthetic_data(model, diffusion, save_path, dataset, num_instances, chunk_size):\n","    n_samples = num_instances - dataset.__len__()\n","    sampled_images = []\n","    for i in range(0, n_samples, chunk_size):\n","        # Sample images in chunks\n","        chunk_size_curr = min(chunk_size, n_samples - i)\n","        with torch.no_grad():\n","            sampled_images.append(diffusion.sample(model, n=chunk_size_curr))\n","    sampled_images = torch.cat(sampled_images, dim=0)\n","    for i, image in enumerate(sampled_images):\n","      array_image = image.permute(1, 2, 0).to('cpu').numpy()\n","      pil_image = Image.fromarray(array_image)\n","      pil_image = transforms.Resize((224, 224))(pil_image)\n","      pil_image.save(os.path.join(save_path, dataset.__getlabel__(0), 'ddpm_'+str(dataset.__getlabel__(0))+'_'+str(i)+'.jpg'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_sXKXUx50ioq"},"source":["## Step 7 - Training the Unconditional DDPM\n","\n","- Defining the training function for the Unconditional DDPM. The code also samples and plot synthetic images in each 10 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-04-16T15:01:07.542806Z","iopub.status.busy":"2023-04-16T15:01:07.542061Z","iopub.status.idle":"2023-04-16T15:44:15.750757Z","shell.execute_reply":"2023-04-16T15:44:15.748862Z","shell.execute_reply.started":"2023-04-16T15:01:07.542762Z"},"id":"7ppEv1HN0ivD","outputId":"22a38339-b67c-4f71-82d3-893cf25507ad","trusted":true},"outputs":[],"source":["def train(epochs, lr, device, dataloader):\n","    losses = []\n","    losses_epoch = []\n","    model = UNet().to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=lr)\n","    mse = nn.MSELoss()\n","    diffusion = Diffusion(img_size=image_size, device=device)\n","    l = len(dataloader)\n","\n","    for epoch in range(epochs):\n","        pbar = tqdm(dataloader)\n","        for i, (images, _) in enumerate(pbar):\n","            images = images.to(device)\n","            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n","            x_t, noise = diffusion.noise_images(images, t)\n","            predicted_noise = model(x_t, t)\n","            loss = mse(noise, predicted_noise)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.set_postfix(MSE=loss.item())\n","\n","            losses.append(loss.item())\n","        \n","        if epoch % 10 == 0:\n","            sampled_images = diffusion.sample(model, n=images.shape[0])\n","            plot_images(sampled_images)\n","\n","        losses_epoch.append(sum(losses)/len(losses))\n","\n","    return model, diffusion, losses_epoch"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Trainning the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model, diffusion, losses_epoch =  train(epochs = epochs, \n","                                  lr = lr, \n","                                  device=device, \n","                                  dataloader=dataloader)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K3IuXQRxLyxe"},"source":["## Step 8 - Plotting the Loss over Epochs\n","\n","- Visualizing the loss of the U-Net architecture over epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"-sKk367UHRMn","outputId":"04e33a5c-37ad-4b1b-9472-4ae151b1632c","trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10,5))\n","plt.title(\"Training loss\")\n","plt.plot(losses_epoch, label=\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RsvsGopcFVa-"},"source":["## Step 9 - Saving the Synthetic Images\n","\n","- Saving images in a desired folder location.\n","- Defining the desired number of images in total after the synthetic augmentation.\n","- As mentioned, the images are created per class since this is a unconditional implementation.\n","- Defining the `chunk_size` for the saving images function."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-16T15:44:16.819036Z","iopub.status.busy":"2023-04-16T15:44:16.818167Z","iopub.status.idle":"2023-04-16T16:20:32.540003Z","shell.execute_reply":"2023-04-16T16:20:32.538989Z","shell.execute_reply.started":"2023-04-16T15:44:16.818990Z"},"id":"Km9qP0XOBAq5","outputId":"c9d264df-0520-407d-c64b-3ba191fffb33","trusted":true},"outputs":[],"source":["save_path = \"/kaggle/working/06_ddpm_uncond\"\n","num_instances = 1000\n","chunk_size = 10\n","\n","save_synthetic_data(model, diffusion, save_path, dataset, num_instances, chunk_size)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["HBohdrcyxgXg"],"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
