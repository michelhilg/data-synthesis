{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Conditional Denoising Diffusion Probabilistic Model (DDPM) - Implementation\n","\n","The purpose of this notebook is to implement the Conditional Denoising Diffusion Probabilistic Models architecture, as outlined in section 3.4.5 of the bachelor thesis.\n","\n","The code provided in this notebook was developed using the Kaggle platform.\n","\n","The code in this notebook incorporates the following sources as references:\n","\n","- https://github.com/dome272/Diffusion-Models-pytorch\n","- https://arxiv.org/pdf/2102.09672.pdf\n","- https://arxiv.org/pdf/2105.05233.pdf"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aVPCcTo3xiu8"},"source":["## Step 1 - Importing Dependencies\n","\n","- Importing the necessary libraries to execute the code."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:56:19.186081Z","iopub.status.busy":"2023-06-03T07:56:19.185632Z","iopub.status.idle":"2023-06-03T07:56:19.198637Z","shell.execute_reply":"2023-06-03T07:56:19.197469Z","shell.execute_reply.started":"2023-06-03T07:56:19.186035Z"},"id":"TVoX2eswxi6M","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as transforms\n","import copy\n","import seaborn as sns\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 2 - Hyperparameter Settings\n","\n","- Set the HPs for the Unconditional DDPM deep generative model. Besides, also check whether a GPU is available for use."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.manual_seed(42)   # Manual seed for running\n","num_classes = 10        # Defining the number of classes since this model follows a conditional approach\n","image_size = 64         # Image size for training the model. It will also be the size of the synthetic images created\n","batch_size = 4          # Batch size to train the model\n","workers = 2             # Number of CPU workers to process the data\n","ngpu = 2                # Number of GPU available\n","noise_steps = 350       # Number of steps in the forward adding noise process\n","epochs = 180            # Number of epochs to train the Unconditional DDPM\n","lr = 1e-5               # Learning rate of the AdamW optimizer\n","\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bOqBxNgNzBKM"},"source":["## Step 3 - Dataset Loading\n","\n","- Defining the custom class for loading the images as PyTorch dataset."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T13:23:51.700440Z","iopub.status.busy":"2023-04-15T13:23:51.699797Z","iopub.status.idle":"2023-04-15T13:23:51.712113Z","shell.execute_reply":"2023-04-15T13:23:51.710079Z","shell.execute_reply.started":"2023-04-15T13:23:51.700399Z"},"id":"lHP60LfGzAnr","trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, labels_file, root_dir, transform=None):\n","        self.annotations = pd.read_csv(labels_file, header=None)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir, str(self.annotations.iloc[index, 1]), self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        image = image.convert(\"RGB\")\n","        label = torch.tensor(int(self.annotations.iloc[(index, 2)]))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return(image, label)\n","    \n","    def __getlabel__(self, index):\n","        label = (self.annotations.iloc[(index, 1)])        \n","\n","        return(label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- The preprocessing transformation matchs the data with the expected format from the DDPM model.\n","- Since this is a conditional model, the labels .cvs file should be passed from the complete dataset, not class-wise."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T13:23:51.715851Z","iopub.status.busy":"2023-04-15T13:23:51.715454Z","iopub.status.idle":"2023-04-15T13:23:51.724542Z","shell.execute_reply":"2023-04-15T13:23:51.722929Z","shell.execute_reply.started":"2023-04-15T13:23:51.715806Z"},"id":"doZ8ts__yv6Q","trusted":true},"outputs":[],"source":["preprocessing = transforms.Compose([transforms.Resize(image_size), \n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                                   ])\n","\n","labels_file = '/path/to/complete/labels/csv'\n","root_dir = '/path/to/root/image/folder'\n","\n","dataset = Dataset(labels_file=labels_file, root_dir=root_dir, transform=preprocessing)\n","dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=workers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gi7FMHHfzC3z"},"source":["## Step 4 - Defining the Diffusion Tools for the Forward Process\n","\n","- Setting the noise schedule and complete forward process in the DDPM architecture.\n","- The forward process is responsible for add noise to the original image based on the noise schedule and the timestep."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:56:28.603531Z","iopub.status.busy":"2023-06-03T07:56:28.602711Z","iopub.status.idle":"2023-06-03T07:56:28.623281Z","shell.execute_reply":"2023-06-03T07:56:28.621780Z","shell.execute_reply.started":"2023-06-03T07:56:28.603489Z"},"id":"t2SdjWBVzDZT","trusted":true},"outputs":[],"source":["class Diffusion:\n","    def __init__(self, noise_steps=noise_steps, beta_start=1e-4, beta_end=0.02, img_size=256, device=device):\n","        self.noise_steps = noise_steps\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.img_size = img_size\n","        self.device = device\n","\n","        self.beta = self.prepare_noise_schedule().to(device)\n","        self.alpha = 1. - self.beta\n","        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n","\n","    def prepare_noise_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n","\n","    def noise_images(self, x, t):\n","        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n","        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n","        Ɛ = torch.randn_like(x)\n","        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n","\n","    def sample_timesteps(self, n):\n","        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n","\n","    def sample(self, model, n, labels, cfg_scale=3):\n","        model.eval()\n","        with torch.no_grad():\n","            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n","            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n","                t = (torch.ones(n) * i).long().to(self.device)\n","                predicted_noise = model(x, t, labels)\n","                if cfg_scale > 0:\n","                    uncond_predicted_noise = model(x, t, None)\n","                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n","                alpha = self.alpha[t][:, None, None, None]\n","                alpha_hat = self.alpha_hat[t][:, None, None, None]\n","                beta = self.beta[t][:, None, None, None]\n","                if i > 1:\n","                    noise = torch.randn_like(x)\n","                else:\n","                    noise = torch.zeros_like(x)\n","                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n","        model.train()\n","        x = (x.clamp(-1, 1) + 1) / 2\n","        x = (x * 255).type(torch.uint8)\n","        return x\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gz9H-hG70mRj"},"source":["## Step 5 - Defining the Reverse Process Neural Network (U-net Architecture)\n","\n","- This code defines the U-Net architecture responsible for the reverse process in the Conditional DDPM model.\n","- This process has the role of removing the noise that was added to the images in the forward process.\n","- Since now is a conditional model, the class of the images are also added to the U-Net model definition."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T07:56:30.341218Z","iopub.status.busy":"2023-06-03T07:56:30.340598Z","iopub.status.idle":"2023-06-03T07:56:30.403218Z","shell.execute_reply":"2023-06-03T07:56:30.401381Z","shell.execute_reply.started":"2023-06-03T07:56:30.341169Z"},"id":"SoqNQ-cc0hat","trusted":true},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, channels, size):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.size = size\n","        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n","        self.ln = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n","        x_ln = self.ln(x)\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n","\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n","        super().__init__()\n","        self.residual = residual\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, mid_channels),\n","            nn.GELU(),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.GroupNorm(1, out_channels),\n","        )\n","\n","    def forward(self, x):\n","        if self.residual:\n","            return F.gelu(x + self.double_conv(x))\n","        else:\n","            return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, t):\n","        x = self.maxpool_conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_dim=256):\n","        super().__init__()\n","\n","        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n","        self.conv = nn.Sequential(\n","            DoubleConv(in_channels, in_channels, residual=True),\n","            DoubleConv(in_channels, out_channels, in_channels // 2),\n","        )\n","\n","        self.emb_layer = nn.Sequential(\n","            nn.SiLU(),\n","            nn.Linear(\n","                emb_dim,\n","                out_channels\n","            ),\n","        )\n","\n","    def forward(self, x, skip_x, t):\n","        x = self.up(x)\n","        x = torch.cat([skip_x, x], dim=1)\n","        x = self.conv(x)\n","        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n","        return x + emb\n","\n","\n","class UNet_conditional(nn.Module):\n","    def __init__(self, c_in=3, c_out=3, time_dim=256, num_classes=None, device=device):\n","        super().__init__()\n","        self.device = device\n","        self.time_dim = time_dim\n","        self.inc = DoubleConv(c_in, 64)\n","        self.down1 = Down(64, 128)\n","        self.sa1 = SelfAttention(128, 32)\n","        self.down2 = Down(128, 256)\n","        self.sa2 = SelfAttention(256, 16)\n","        self.down3 = Down(256, 256)\n","        self.sa3 = SelfAttention(256, 8)\n","\n","        self.bot1 = DoubleConv(256, 512)\n","        self.bot2 = DoubleConv(512, 512)\n","        self.bot3 = DoubleConv(512, 256)\n","\n","        self.up1 = Up(512, 128)\n","        self.sa4 = SelfAttention(128, 16)\n","        self.up2 = Up(256, 64)\n","        self.sa5 = SelfAttention(64, 32)\n","        self.up3 = Up(128, 64)\n","        self.sa6 = SelfAttention(64, 64)\n","        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n","\n","        if num_classes is not None:\n","            self.label_emb = nn.Embedding(num_classes, time_dim)\n","\n","    def pos_encoding(self, t, channels):\n","        inv_freq = 1.0 / (\n","            10000\n","            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n","        )\n","        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n","        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n","        return pos_enc\n","\n","    def forward(self, x, t, y):\n","        t = t.unsqueeze(-1).type(torch.float)\n","        t = self.pos_encoding(t, self.time_dim)\n","\n","        if y is not None:\n","            t += self.label_emb(y)\n","\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1, t)\n","        x2 = self.sa1(x2)\n","        x3 = self.down2(x2, t)\n","        x3 = self.sa2(x3)\n","        x4 = self.down3(x3, t)\n","        x4 = self.sa3(x4)\n","\n","        x4 = self.bot1(x4)\n","        x4 = self.bot2(x4)\n","        x4 = self.bot3(x4)\n","\n","        x = self.up1(x4, x3, t)\n","        x = self.sa4(x)\n","        x = self.up2(x, x2, t)\n","        x = self.sa5(x)\n","        x = self.up3(x, x1, t)\n","        x = self.sa6(x)\n","        output = self.outc(x)\n","        return output"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 6 - Defining the Exponential Moving Average (EMA) Algorithm\n","\n","- As a State-of-the-art model, this Conditional DDPM uses the concept of EMA for perform a stable training.\n","- The EMA class is defined below and use in the training function for the Conditional DDPM definition."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EMA:\n","    def __init__(self, beta):\n","        super().__init__()\n","        self.beta = beta\n","        self.step = 0\n","\n","    def update_model_average(self, ma_model, current_model):\n","        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","            old_weight, up_weight = ma_params.data, current_params.data\n","            ma_params.data = self.update_average(old_weight, up_weight)\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","        return old * self.beta + (1 - self.beta) * new\n","\n","    def step_ema(self, ema_model, model, step_start_ema=2000):\n","        if self.step < step_start_ema:\n","            self.reset_parameters(ema_model, model)\n","            self.step += 1\n","            return\n","        self.update_model_average(ema_model, model)\n","        self.step += 1\n","\n","    def reset_parameters(self, ema_model, model):\n","        ema_model.load_state_dict(model.state_dict())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mmDR3XRA2vW8"},"source":["## Step 7 - Utils Functions for Visualization and Saving\n","\n","- `plot_images` - Function to plot the synthetic data over the trainning of the model. Util to visualize the evolution of the images over epochs.\n","- `save_synthetic_data` - Function to save the synthetic images. Work in chunks to save RAM memory since the model has to synthetize and perform the reverse process for each image that is creating."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T16:28:55.386756Z","iopub.status.busy":"2023-04-15T16:28:55.385693Z","iopub.status.idle":"2023-04-15T16:28:55.399155Z","shell.execute_reply":"2023-04-15T16:28:55.398099Z","shell.execute_reply.started":"2023-04-15T16:28:55.386714Z"},"id":"eL9yFR2W2vdV","trusted":true},"outputs":[],"source":["def plot_images(images):\n","    plt.figure(figsize=(32, 32))\n","    plt.imshow(torch.cat([\n","        torch.cat([i for i in images.cpu()], dim=-1),\n","    ], dim=-2).permute(1, 2, 0).cpu())\n","    plt.show()\n","\n","\n","def save_synthetic_data(model, diffusion, save_path, num_instances, label, chunk_size, label_name):\n","    labels = torch.ones(num_instances, dtype=torch.long) * label\n","    labels = labels.to(device)\n","    sampled_images = []\n","    for i in range(0, num_instances, chunk_size):\n","        # Sample images in chunks\n","        chunk_size_curr = min(chunk_size, num_instances - i)\n","        with torch.no_grad():\n","            sampled_images.append(diffusion.sample(model, n=chunk_size_curr, labels=labels[i:i+chunk_size_curr])) \n","    sampled_images = torch.cat(sampled_images, dim=0)\n","    for i, image in enumerate(sampled_images):\n","      array_image = image.permute(1, 2, 0).to('cpu').numpy()\n","      pil_image = Image.fromarray(array_image)\n","      pil_image = transforms.Resize((224, 224))(pil_image)\n","      pil_image.save(os.path.join(save_path, label_name, 'ddpm_cond_'+label_name+'_'+str(i)+'.jpg'))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_sXKXUx50ioq"},"source":["## Step 8 - Training the Unconditional DDPM\n","\n","- Defining the training function for the Conditional DDPM. The code also samples and plot synthetic images in each 10 epochs.\n","- Since follows the conditional defintion, the training loop will sample one synthetic instance per class."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.execute_input":"2023-04-15T13:23:51.875145Z","iopub.status.busy":"2023-04-15T13:23:51.874869Z","iopub.status.idle":"2023-04-15T16:10:41.933051Z","shell.execute_reply":"2023-04-15T16:10:41.931647Z","shell.execute_reply.started":"2023-04-15T13:23:51.875118Z"},"id":"7ppEv1HN0ivD","outputId":"68a6d55f-8466-49bb-ffbd-8e4ee5d60640","trusted":true},"outputs":[],"source":["def train(epochs, lr, device, dataloader, num_classes):\n","    losses = []\n","    losses_epoch = []\n","    model = UNet_conditional(num_classes=num_classes).to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=lr)\n","    mse = nn.MSELoss()\n","    diffusion = Diffusion(img_size=image_size, device=device)\n","    l = len(dataloader)\n","    ema = EMA(0.995)\n","    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n","\n","\n","    for epoch in range(epochs):\n","        pbar = tqdm(dataloader)\n","        for i, (images, labels) in enumerate(pbar):\n","            images = images.to(device)\n","            labels = labels.to(device)            \n","            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n","            x_t, noise = diffusion.noise_images(images, t)\n","            if np.random.random() < 0.1:\n","                labels = None            \n","            predicted_noise = model(x_t, t, labels)\n","            loss = mse(noise, predicted_noise)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            ema.step_ema(ema_model, model)\n","\n","            pbar.set_postfix(MSE=loss.item())\n","\n","            losses.append(loss.item())\n","\n","        if epoch % 10 == 0: \n","            labels = torch.arange(10).long().to(device)\n","            sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n","            plot_images(sampled_images)\n","        \n","        losses_epoch.append(sum(losses)/len(losses))\n","\n","    return model, diffusion, losses_epoch"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Training the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model, diffusion, losses_epoch =  train(epochs = epochs, \n","                                  lr = lr, \n","                                  device=device, \n","                                  dataloader=dataloader, \n","                                  num_classes=num_classes)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kb5iJeQOQHMo"},"source":["## Step 9 - Plotting the Loss over Epochs\n","\n","- Visualizing the loss of the U-Net architecture over epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"execution":{"iopub.execute_input":"2023-04-15T17:56:14.516326Z","iopub.status.busy":"2023-04-15T17:56:14.515726Z","iopub.status.idle":"2023-04-15T17:56:14.774577Z","shell.execute_reply":"2023-04-15T17:56:14.773549Z","shell.execute_reply.started":"2023-04-15T17:56:14.516277Z"},"id":"qCll0virtuYC","outputId":"2abf7433-52ef-4a0f-fa47-d8bb98ec2c4f","trusted":true},"outputs":[],"source":["sns.set_theme(style=\"whitegrid\")\n","plt.figure(figsize=(10,5))\n","plt.title(\"Training loss\")\n","plt.plot(losses_epoch, label=\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8K1FhXjgOq2r"},"source":["## Step 9 - Saving the Synthetic Images\n","\n","- Saving images in a desired folder location, the images are saved in a class-wise manner.\n","- The `label` especify the desired class.\n","- The `target` dictionary define the desired number of instances per class\n","- Defining the `chunk_size` for the saving images function.\n","- **OBS.:** Is necessary to have a subdir with the class name inside the `save_path` dir."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-15T19:23:59.616934Z","iopub.status.busy":"2023-04-15T19:23:59.616491Z","iopub.status.idle":"2023-04-15T20:48:28.955159Z","shell.execute_reply":"2023-04-15T20:48:28.954140Z","shell.execute_reply.started":"2023-04-15T19:23:59.616897Z"},"id":"qfretZ6mPTN7","outputId":"c8fdd09d-f42b-4927-a60b-32507d8c25f9","trusted":true},"outputs":[],"source":["discrete_class = {\n","    0: \"0_punching_hole\",\n","    1: \"1_welding_line\",\n","    2: \"2_crescent_gap\",\n","    3: \"3_water_spot\",\n","    4: \"4_oil_spot\",\n","    5: \"5_silk_spot\",\n","    6: \"6_inclusion\",\n","    7: \"7_rolled_pit\",\n","    8: \"8_crease\",\n","    9: \"9_waist_folding\"\n","}\n","\n","target = {0: 860, 1: 826, 2: 856, 3: 816, 4: 870, 5: 584, 6: 862, 7: 981, 8: 967, 9: 904}\n","\n","\n","save_path = \"/root/path/to/save/the/images\"     # Root path to save the images, contains subdir for each class equals to the class name.\n","label = 2                                       # Enter the discrete number for this class.\n","chunk_size = 10       \n","\n","save_synthetic_data(model, diffusion, save_path, target[label], label, chunk_size, discrete_class[label])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jtZhyK28u8W1","3y0GvgoafxRu"],"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
