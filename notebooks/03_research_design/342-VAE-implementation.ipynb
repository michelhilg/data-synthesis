{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Convolutional Variational Autoencoder (VAE) - Implementation\n","\n","The purpose of this notebook is to implement the Convolutional Variational Autoencoder architecture, as outlined in section 3.4.2 of the bachelor thesis.\n","\n","The code provided in this notebook was developed using the Google Colab platform.\n","\n","The code in this notebook incorporates the following sources as references:\n","\n","- https://github.com/AntixK/PyTorch-VAE\n","- https://medium.com/dataseries/variational-autoencoder-with-pytorch-2d359cbf027b"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7X3ok5bwAJbr"},"source":["## Step 1 - Importing Dependencies\n","\n","- Importing the necessary libraries to execute the code."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1681578626314,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"gtjESpOmAK11"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd \n","import torch\n","import torchvision\n","from torchvision import transforms\n","from torch import nn\n","import torch.nn.functional as F"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b1jd8byhFOfb"},"source":["## Step 2 - Hyperparameter Settings\n","\n","- Set the HPs for the Convolutional VAE deep generative model. Besides, also check whether a GPU is available for use."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1681577772417,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"iLxFsIR9FNY2","outputId":"2b04e860-e3ac-48cf-ab41-e596163f9bcb"},"outputs":[],"source":["batch_size = 4          # Batch size for the Conv. VAE training\n","image_size = 64         # Image resolution for Conv. VAE training and output\n","num_workers = 2         # Number of CPU workers to process the data\n","d = 4                   # Latent Dimension\n","lr = 1e-4               # Learning rate for the Adam optimizer \n","num_epochs = 800        # Number of epochs that the Conv. VAE will be trained\n","weight_decay=1e-5       # Weight decay for the Adam optimizer\n","torch.manual_seed(42)   # Manual seed definition\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(f'Selected device: {device}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IraG6GNwARH1"},"source":["## Step 3 - Dataset Loading\n","\n","- Defining the custom class for loading the images as PyTorch dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y8wqAa1ASlT"},"outputs":[],"source":["class Dataset(Dataset):\n","    def __init__(self, labels_file, root_dir, transform=None):\n","        self.annotations = pd.read_csv(labels_file, header=None)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir, str(self.annotations.iloc[index, 1]), self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        label = torch.tensor(int(self.annotations.iloc[(index, 2)]))\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return(image, label)\n","    \n","    def __getlabel__(self, index):\n","        label = (self.annotations.iloc[(index, 1)])        \n","\n","        return(label)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- The preprocessing transformation matchs the data with the expected format from the Conv. VAE model.\n","- The labels .cvs file should be passed in a class-wise definiton, since the model is unconditional will generate one class per time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uz-0KCUaElLd"},"outputs":[],"source":["preprocessing = transforms.Compose([transforms.Resize(image_size), \n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0,), (1,))\n","                                   ])\n","\n","labels_file = '/path/to/class/labels/csv'\n","root_dir = '/path/to/root/image/folder'\n","\n","dataset = Dataset(labels_file=labels_file, root_dir=root_dir, transform=preprocessing)\n","dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DjIRL0zLASvD"},"source":["## Step 4 - Convolutional VAE Model Definition\n","\n","- **Encoder:** Defining the encoder of the VAE model, the encoder has the role of mapping the image data distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1681577887878,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"25qcmW_mQhUh"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, latent_dims):\n","        super(Encoder, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n","        self.batch1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n","        self.batch2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n","        self.batch3 = nn.BatchNorm2d(128)\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=0)\n","        self.batch4 = nn.BatchNorm2d(256)\n","        self.linear1 = nn.Linear (256 * 3 * 3, 1152)\n","        self.linear2 = nn.Linear(1152, latent_dims)\n","        self.linear3 = nn.Linear(1152, latent_dims)\n","\n","        self.N = torch.distributions.Normal(0, 1)\n","\n","        if torch.cuda.is_available():\n","          self.N.loc = self.N.loc.cuda()\n","          self.N.scale = self.N.scale.cuda()\n","\n","        self.kl = 0\n","\n","\n","    def forward(self, x):\n","\n","        x = x.to(device)\n","        x = F.leaky_relu(self.batch1(self.conv1(x)))\n","        x = F.leaky_relu(self.batch2(self.conv2(x)))\n","        x = F.leaky_relu(self.batch3(self.conv3(x)))\n","        x = F.leaky_relu(self.batch4(self.conv4(x)))\n","        x = torch.flatten(x, start_dim=1)\n","        x = F.leaky_relu(self.linear1(x))\n","        mu =  self.linear2(x)\n","        sigma = torch.exp(self.linear3(x))\n","        z = mu + sigma*self.N.sample(mu.shape)\n","\n","        self.kl = -0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n","\n","        return z      "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- **Decoder:** Defining the decoder of the VAE implementation, the decoder has the role of learning the original image distribution and, based on that, reconstructing new data instances that belong to the same original distribution."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","    \n","    def __init__(self, latent_dims):\n","        super().__init__()\n","\n","        self.decoder_lin = nn.Sequential(\n","            nn.Linear(latent_dims, 1152),\n","            nn.LeakyReLU(),\n","            nn.Linear(1152, 256 * 3 * 3),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256, 3, 3))\n","\n","        self.decoder_conv = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, 4, stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(),\n","            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(),\n","            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.LeakyReLU(),\n","            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)\n","        )\n","        \n","    def forward(self, x):\n","        x = self.decoder_lin(x)\n","        x = self.unflatten(x)\n","        x = self.decoder_conv(x)\n","        x = torch.sigmoid(x)\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Defining the Convolutional VAE complete class."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1681577888113,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"qKT5HdlDAJoU"},"outputs":[],"source":["class VariationalAutoencoder(nn.Module):\n","    def __init__(self, latent_dims):\n","        super(VariationalAutoencoder, self).__init__()\n","        self.encoder = Encoder(latent_dims)\n","        self.decoder = Decoder(latent_dims)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        z = self.encoder(x)\n","        return self.decoder(z)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QQ_PBtodB4A2"},"source":["## Step 5 - Training the Model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Initializing the Convolutional VAE model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1681577892454,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"30Uvq_YMANsl","outputId":"4ae765b2-d30f-4b00-d5c1-63d07f37127b"},"outputs":[],"source":["vae = VariationalAutoencoder(latent_dims=d)\n","optim = torch.optim.Adam(vae.parameters(), lr=lr, weight_decay=weight_decay)\n","vae.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Training function for the Convolutional VAE model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ccjdexc3Pv8t"},"outputs":[],"source":["def train_epoch(vae, device, dataloader, optimizer, loss_fn):\n","    vae.train()\n","    train_loss = 0.0\n","    \n","    for x, _ in dataloader: \n","        x = x.to(device)\n","        x_hat = vae(x)\n","\n","        reconst_loss = loss_fn(x_hat, x)\n","        loss = reconst_loss + vae.encoder.kl\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss+=loss.item()\n","\n","    return train_loss / len(dataloader.dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Training the Convolutional VAE model, the train loss is obtained and will be also displayed later."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":617635,"status":"ok","timestamp":1681452589210,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"XZGKuwMGBdrd","outputId":"9558eeca-762d-4439-be50-a028c82946db"},"outputs":[],"source":["train_loss_log = []\n","loss_fn = nn.BCELoss(reduction=\"sum\")\n","\n","for epoch in range(num_epochs):\n","   train_loss = train_epoch(vae, device, dataloader, optim, loss_fn)\n","   train_loss_log.append(train_loss)\n","   print('\\n EPOCH {}/{} \\t train loss {:.3f}'.format(epoch + 1, num_epochs,train_loss))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AQdI6LhNYxb5"},"source":["## Step 6 - Visualizing the Results"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- **Training Results:** Train loss over epochs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.title(\"Loss During Training\")\n","plt.plot(train_loss_log)\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- **Synthetic Images:** Visualizing synthetic data generated by the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":835},"executionInfo":{"elapsed":2342,"status":"ok","timestamp":1681452591540,"user":{"displayName":"Michel Hilgemberg","userId":"08146869376198108831"},"user_tz":-120},"id":"uivgn8e_Ism6","outputId":"52557275-20e7-4081-d8ec-84246331fb36"},"outputs":[],"source":["def show_image(img):\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","vae.eval()\n","\n","with torch.no_grad():\n","\n","    latent = torch.randn(128, d, device=device)\n","    \n","    img_recon = vae.decoder(latent)\n","    img_recon = img_recon.cpu()\n","\n","    fig, ax = plt.subplots(figsize=(15, 10))\n","    show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5))\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Mxk7DzoAY2Gw"},"source":["## Step 7 - Saving the Synthetic Images\n","\n","- Defining a function to create a new images based on a noise input.\n","- Saving images in a desired folder location.\n","- Defining the desired number of images in total after the synthetic augmentation\n","- As mentioned, the images are created per class since this is a unconditional implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLVNUvAQVgr_"},"outputs":[],"source":["def save_synthetic_data(dataset, save_path, num_instances, vae):\n","    num_images = num_instances - dataset.__len__()\n","    vae.eval()\n","    with torch.no_grad():\n","        latent = torch.randn(num_images, d, device=device)\n","\n","        img_recon = vae.decoder(latent)\n","        img_recon = img_recon.cpu()\n","\n","        for i in range(num_images):\n","            tensor_image = img_recon[i].detach().cpu()\n","            pil_image = transforms.ToPILImage()(tensor_image)\n","            pil_image = transforms.Resize((224, 224))(pil_image)\n","            pil_image.save(os.path.join(save_path, dataset.__getlabel__(0), 'vae_'+str(dataset.__getlabel__(0))+'_'+str(i)+'.jpg'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDFz1ifuhctW"},"outputs":[],"source":["save_path = \"/path/to/save/the/images\"\n","num_instances = 1000\n","save_synthetic_data(dataset, save_path, num_instances, vae)"]}],"metadata":{"colab":{"collapsed_sections":["1knERzfuFiPz"],"provenance":[{"file_id":"1SFQ7Lj9PmIduWBBCXuayAaZRO7tNp0ce","timestamp":1679250172712}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
